# CUDA Implementation Guide - VoxSynopsis

## Overview

This document provides a comprehensive guide to the CUDA acceleration implementation in VoxSynopsis, covering both FFmpeg hardware acceleration and FastWhisper GPU processing.

## Architecture

### CUDA Integration Components

```
VoxSynopsis CUDA Architecture
‚îú‚îÄ‚îÄ core/ffmpeg_cuda.py           # FFmpeg CUDA optimization engine
‚îú‚îÄ‚îÄ core/performance.py           # Hardware detection and configuration
‚îú‚îÄ‚îÄ core/settings_dialog.py       # CUDA configuration interface
‚îú‚îÄ‚îÄ core/transcription.py         # CUDA-accelerated transcription
‚îú‚îÄ‚îÄ config.json                   # CUDA runtime configuration
‚îî‚îÄ‚îÄ test_cuda.py                  # CUDA validation and benchmarking
```

## Implementation Details

### 1. FFmpeg CUDA Acceleration (`core/ffmpeg_cuda.py`)

#### FFmpegCUDADetector Class
Provides comprehensive CUDA capability detection:

```python
class FFmpegCUDADetector:
    def is_cuda_available(self) -> bool
    def get_supported_decoders(self) -> Set[str]
    def get_supported_encoders(self) -> Set[str]
    def get_optimal_decoder(self, codec: str) -> Optional[str]
    def test_cuda_decode(self) -> bool
```

**Supported Decoders:**
- `h264_cuvid`: H.264 hardware decoding
- `hevc_cuvid`: HEVC/H.265 hardware decoding
- `av1_cuvid`: AV1 hardware decoding
- `vp8_cuvid`, `vp9_cuvid`: VP8/VP9 hardware decoding
- `mpeg2_cuvid`, `mpeg4_cuvid`: MPEG hardware decoding

**Supported Encoders:**
- `h264_nvenc`: H.264 hardware encoding
- `hevc_nvenc`: HEVC/H.265 hardware encoding
- `av1_nvenc`: AV1 hardware encoding

#### FFmpegCUDAOptimizer Class
Optimizes FFmpeg commands with CUDA acceleration:

```python
class FFmpegCUDAOptimizer:
    def optimize_audio_extraction_cmd(self, input_file, output_file, sample_rate, channels)
    def optimize_audio_tempo_cmd(self, input_file, output_file, tempo_factor)
    def optimize_audio_chunking_cmd(self, input_file, output_file, start_time, duration)
    def optimize_silence_detection_cmd(self, input_file, threshold_db, min_duration)
```

### 2. FastWhisper CUDA Integration

#### Configuration Management
CUDA settings are managed through `config.json`:

```json
{
    "device": "cuda",                    // CPU or CUDA
    "compute_type": "int8",              // int8, float16, int8_float16
    "model_size": "base",                // Model size selection
    "enable_model_caching": true,        // GPU memory caching
    "batch_processing": true             // GPU batch optimization
}
```

#### Hardware Compatibility
- **GTX 1050 Ti**: Supports `int8` compute type only
- **RTX Series**: Supports `int8`, `float16`, `int8_float16`
- **Professional Cards**: Full feature support

### 3. Performance Monitoring Integration

#### Enhanced Status Display
```python
def print_optimization_status():
    device_info = f"üì± Device: {device.upper()}"
    if device == "cuda":
        gpu_name = torch.cuda.get_device_name(0)
        device_info += f" ({gpu_name}) | {compute_type}"
```

**Status Messages:**
- `üì± Device: CUDA (NVIDIA GeForce GTX 1050 Ti) | int8`
- `üì± Device: CPU | int8`

## CUDA Acceleration Points

### 1. Audio Extraction from MP4
**Before (CPU):**
```bash
ffmpeg -hwaccel auto -threads 0 -i video.mp4 -vn -acodec pcm_s16le audio.wav
```

**After (CUDA):**
```bash
ffmpeg -hwaccel cuda -c:v h264_cuvid -hwaccel_output_format cuda -threads 0 -i video.mp4 -vn -acodec pcm_s16le audio.wav
```

### 2. Audio Chunk Creation
**Before (CPU):**
```bash
ffmpeg -threads 0 -i audio.wav -ss 0 -t 30 chunk.wav
```

**After (CUDA):**
```bash
ffmpeg -hwaccel cuda -threads 0 -i audio.wav -ss 0 -t 30 chunk.wav
```

### 3. Audio Tempo Acceleration
**Before (CPU):**
```bash
ffmpeg -threads 0 -i audio.wav -filter:a atempo=1.25 fast.wav
```

**After (CUDA):**
```bash
ffmpeg -hwaccel cuda -threads 0 -i audio.wav -filter:a atempo=1.25 fast.wav
```

### 4. Silence Detection
**Before (CPU):**
```bash
ffmpeg -threads 0 -i audio.wav -af silencedetect=noise=-40dB:d=0.5 -f null -
```

**After (CUDA):**
```bash
ffmpeg -hwaccel cuda -threads 0 -i audio.wav -af silencedetect=noise=-40dB:d=0.5 -f null -
```

## Performance Benchmarks

### FFmpeg Operations (GTX 1050 Ti)
| Operation | CPU Time | CUDA Time | Speedup |
|-----------|----------|-----------|---------|
| Audio Extraction (10s video) | 0.23s | 0.25s | 0.9x |
| Audio Tempo (1.25x) | 0.24s | 0.19s | 1.3x |
| Audio Chunking (6 chunks) | 1.50s | 1.39s | 1.1x |
| Silence Detection | Variable | ~5-10% faster | 1.1x |

### FastWhisper Operations
| Model | Compute Type | Performance Gain |
|-------|--------------|------------------|
| tiny | int8 | 2-3x faster |
| base | int8 | 2-4x faster |
| small | int8 | 3-5x faster |

## Implementation Code Locations

### Core Files Modified

#### 1. `core/transcription.py`
```python
# Audio extraction optimization
from .ffmpeg_cuda import ffmpeg_cuda_optimizer
extract_cmd = ffmpeg_cuda_optimizer.optimize_audio_extraction_cmd(
    media_path, extracted_wav_path, sample_rate=16000, channels=1
)

# Chunk acceleration optimization  
accel_cmd = ffmpeg_cuda_optimizer.optimize_audio_tempo_cmd(
    chunk_path, accelerated_chunk_path, acceleration_factor
)

# Audio chunking optimization
command = ffmpeg_cuda_optimizer.optimize_audio_chunking_cmd(
    filepath, chunk_filepath, start_time, current_chunk_duration, 
    sample_rate=16000, channels=1
)

# Silence detection optimization
silence_cmd = ffmpeg_cuda_optimizer.optimize_silence_detection_cmd(
    filepath, threshold_db=threshold, min_duration=duration
)
```

#### 2. `core/performance.py`
```python
# Enhanced status display with CUDA information
device_info = f"üì± Device: {device.upper()}"
if device == "cuda":
    if cuda_available:
        gpu_name = torch.cuda.get_device_name(0)
        device_info += f" ({gpu_name})"
    else:
        device_info += " (‚ö†Ô∏è Not available - falling back to CPU)"
device_info += f" | {compute_type}"
```

#### 3. `config.json`
```json
{
    "device": "cuda",
    "compute_type": "int8"
}
```

## Testing and Validation

### Automated Tests
- `test_cuda.py`: Comprehensive CUDA capability testing
- `test_ffmpeg_cuda.py`: FFmpeg acceleration benchmarking
- `test_chunking_cuda.py`: Chunking performance validation

### Manual Validation
1. **Hardware Detection**: Verify GPU model detection
2. **Capability Testing**: Confirm decoder/encoder availability
3. **Performance Benchmarking**: Measure actual speedup
4. **Fallback Testing**: Ensure CPU fallback works
5. **Error Handling**: Test CUDA failure scenarios

## Hardware Requirements

### Minimum Requirements
- NVIDIA GPU with CUDA Compute Capability 6.1+
- CUDA Driver 11.0+
- 2GB VRAM minimum

### Recommended Configuration
- GTX 1060 / RTX 2060 or better
- 4GB+ VRAM
- CUDA Driver 12.0+
- FFmpeg with CUDA support

### Supported Compute Types by Hardware
| GPU Architecture | int8 | float16 | int8_float16 |
|------------------|------|---------|--------------|
| Pascal (GTX 10xx) | ‚úÖ | ‚ùå | ‚ùå |
| Turing (RTX 20xx) | ‚úÖ | ‚úÖ | ‚úÖ |
| Ampere (RTX 30xx) | ‚úÖ | ‚úÖ | ‚úÖ |
| Ada (RTX 40xx) | ‚úÖ | ‚úÖ | ‚úÖ |

## Error Handling and Fallbacks

### Graceful Degradation
1. **CUDA Unavailable**: Automatic fallback to CPU
2. **Unsupported Compute Type**: Fallback to compatible type
3. **VRAM Insufficient**: Reduce batch size or model size
4. **Driver Issues**: Detailed error logging and CPU fallback

### Error Messages
```python
# Informative error handling
logger.warning("CUDA not available, using CPU mode")
logger.info("Using CUDA decoder: h264_cuvid for codec: h264")
logger.error("CUDA memory insufficient, falling back to CPU")
```

## Future Enhancements

### Planned Improvements
1. **Dynamic Memory Management**: Automatic VRAM optimization
2. **Multi-GPU Support**: Distribution across multiple GPUs
3. **Advanced Profiling**: Detailed performance metrics
4. **Thermal Monitoring**: GPU temperature and throttling detection

### Performance Optimizations
1. **Memory Pooling**: Reduce allocation overhead
2. **Async Processing**: Overlap CPU and GPU operations
3. **Batch Optimization**: Dynamic batch sizing
4. **Pipeline Parallelism**: Multi-stage GPU pipeline

## Troubleshooting

### Common Issues
1. **CUDA Not Detected**: Check driver installation
2. **OutOfMemory Errors**: Reduce model size or batch size
3. **Slow Performance**: Verify CUDA acceleration is active
4. **Compatibility Issues**: Check compute capability

### Debug Commands
```bash
# Test CUDA availability
python3 test_cuda.py

# Benchmark FFmpeg CUDA
python3 test_ffmpeg_cuda.py

# Validate chunking performance
python3 test_chunking_cuda.py
```

## Configuration Best Practices

### Optimal Settings by Hardware
```python
# GTX 1050 Ti (4GB VRAM)
{
    "device": "cuda",
    "compute_type": "int8",
    "model_size": "base",
    "batch_size": 4
}

# RTX 3060 (12GB VRAM)  
{
    "device": "cuda",
    "compute_type": "float16",
    "model_size": "medium",
    "batch_size": 8
}

# RTX 4090 (24GB VRAM)
{
    "device": "cuda", 
    "compute_type": "float16",
    "model_size": "large-v3",
    "batch_size": 16
}
```

## Conclusion

The CUDA implementation in VoxSynopsis provides comprehensive GPU acceleration across both FFmpeg operations and FastWhisper transcription. The system includes robust hardware detection, graceful fallbacks, and automatic optimization while maintaining compatibility with a wide range of NVIDIA GPUs.

The modular design allows for easy extension and maintenance while providing significant performance improvements for users with compatible hardware.